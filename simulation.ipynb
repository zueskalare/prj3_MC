{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 修改dat file赋予bond和dihedial势能\n",
    "\n",
    "import os\n",
    "import sys\n",
    "import fileinput\n",
    "\n",
    "di_strenght = [0.2,0.4,0.6,0.8,1.0]\n",
    "in_dir = 'code/MC_model/python/spinodal_model' # output directory\n",
    "for ds in di_strenght:\n",
    "    out_dir = f'spinodal_model_dat_{ds}' # input directory\n",
    "    \n",
    "    os.system('mkdir -p %s' % out_dir)\n",
    "    \n",
    "    trilinic = True\n",
    "    bond = '\\t1 0.959691 1.88122 3.11032 -0.91359 5.0\\n'\n",
    "    dihedral = f'\\t1 {ds} 1 1\\n'\n",
    "    for root, dirs, files in os.walk(in_dir):\n",
    "        for file in files:\n",
    "            if file.endswith('.dat')and file[0]!='.':\n",
    "                in_f = os.path.join(root, file)\n",
    "                out_f = os.path.join(out_dir, file)\n",
    "                print(in_f,out_f)\n",
    "                fin = open(in_f, 'r')\n",
    "                fout = open(out_f, 'w')\n",
    "                for id,line in enumerate(fin.readlines(),1):# 25 for bond and 29 for dihedral\n",
    "                    # print(line)\n",
    "                    if trilinic:\n",
    "                        if id == 17:\n",
    "                            line = line+'0.0 0.0 0.0 xy xz yz\\n'\n",
    "                    if id == 25:\n",
    "                        line = bond\n",
    "                    elif id == 29:\n",
    "                        line = dihedral\n",
    "                    fout.write(line)\n",
    "                fin.close()\n",
    "                fout.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simulate \n",
    "\n",
    "infile_dir = 'infile'\n",
    "data_dir_ = 'spinodal_model_dat'\n",
    "\n",
    "di_strenght = [0.2,0.4,0.6,0.8,1.0]\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "for ds in di_strenght:\n",
    "    data_dir= data_dir_ +f'_{ds}'\n",
    "    for iroot,idirs,ifiles in os.walk(infile_dir):\n",
    "        if len(iroot.split('/'))!=1:\n",
    "            continue\n",
    "        for ifile in ifiles:\n",
    "            if ifile[0] != '.' and 'compress' in ifile:\n",
    "                in_file = os.path.join(iroot,ifile)\n",
    "                c = 0\n",
    "                for droot, ddirs,dfiles in os.walk(data_dir):\n",
    "                    for dfile in dfiles:\n",
    "                        if dfile[0] != '.' and dfile.endswith('.dat'):\n",
    "                            data = os.path.join(droot,dfile)[:-4]\n",
    "                            data = os.path.abspath(data)\n",
    "                            o_droot= os.path.abspath(droot)\n",
    "                            o_droot = o_droot.replace('home','scratch')\n",
    "                            o_data = os.path.join(o_droot+'/test75',dfile)[:-4]\n",
    "                            os.system(f'mkdir -p {o_droot+\"/test75\"}')\n",
    "                            c+=1\n",
    "                            cmd = f'mpirun -np 64 tian_jie_lmp -i {in_file} -var file {data} -var ofile {o_data}'\n",
    "                            print(cmd)\n",
    "                            # os.system(cmd)\n",
    "                            break\n",
    "                            if c>=10:\n",
    "                                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# run simulation on cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7695\n"
     ]
    }
   ],
   "source": [
    "# get simulation batch file \n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "job_name = 'Simulation'\n",
    "\n",
    "infile_dir = 'infile'\n",
    "data_dir_ = 'spinodal_model_dat'\n",
    "\n",
    "# di_strenght = [0.2,0.4,0.6,0.8,1.0]\n",
    "di_strenght = [0.2,0.6,1.0]\n",
    "\n",
    "\n",
    "bf = open(f'batch_files/{job_name}.sh','w')\n",
    "bf.write(f'''#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=batch   # Partition name (batch, highmem_p, or gpu_p)\n",
    "##SBATCH --gres=gpu:A100:1\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=64\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=128G\n",
    "#SBATCH --time=2-00:00:00\n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --mail-user=jt35560@uga.edu   # Mair eves BOIN, END, FAIL, ALL)\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=/home/jt35560/tmp/slurm_log/%j-%x-%N.out\n",
    "\n",
    "source /home/jt35560/.bashrc\n",
    "source /home/jt35560/.local/module/lmp.sh\n",
    "conda activate py311 || exit\n",
    "ml OpenMPI\n",
    "\n",
    "cd /home/jt35560/codebase/prj3/batch_files\n",
    "\n",
    "# Log file to keep track of completed jobs\n",
    "LOGFILE=\"{job_name}_log.txt\"\n",
    "# Check if log file exists, if not, create one\n",
    "if [[ ! -f \"$LOGFILE\" ]]; then\n",
    "    touch \"$LOGFILE\"\n",
    "fi\n",
    "\n",
    "# Output log for the jobs\n",
    "OUTPUT_LOG=\"{job_name}_output_log.txt\"\n",
    "\n",
    "if [[ ! -f \"$OUTPUT_LOG\" ]]; then\n",
    "    touch \"$OUTPUT_LOG\"\n",
    "fi\n",
    "\n",
    "declare -a jobs=(\n",
    "''')\n",
    "\n",
    "job_num = 0\n",
    "import os\n",
    "comp = 25\n",
    "for ds in di_strenght:\n",
    "    data_dir= data_dir_ +f'_{ds}'\n",
    "    for iroot,idirs,ifiles in os.walk(infile_dir):\n",
    "        if len(iroot.split('/'))!=1:\n",
    "            continue\n",
    "        for ifile in ifiles:\n",
    "            if ifile[0] != '.' and 'compress' in ifile and not('relax' in ifile):\n",
    "                in_file = os.path.join(iroot,ifile)\n",
    "                in_file = os.path.abspath(in_file)\n",
    "                for droot, ddirs,dfiles in os.walk(data_dir):\n",
    "                    for dfile in dfiles:\n",
    "                        if dfile[0] != '.' and dfile.endswith('.dat'):\n",
    "                            data = os.path.join(droot,dfile)[:-4]\n",
    "                            data = os.path.abspath(data)\n",
    "                            o_droot= os.path.abspath(droot)\n",
    "                            o_droot = o_droot.replace('home','scratch')\n",
    "                            o_data = os.path.join(o_droot+f'/{job_name}',dfile)[:-4]\n",
    "                            os.system(f'mkdir -p {o_droot+f\"/{job_name}\"}')\n",
    "                            c+=1\n",
    "                            cmd = f'mpirun -np 64 tian_jie_lmp -i {in_file} -var file {data} -var ofile {o_data} -var comp {comp}'\n",
    "                            # print(cmd)\n",
    "                            bf.write(\"\\\"\"+cmd+\"\\\"\\n\")\n",
    "                            job_num+=1\n",
    "                            # os.system(cmd)\n",
    "\n",
    "                                    \n",
    "bf.write(')\\n')\n",
    "\n",
    "\n",
    "bf.write('''\n",
    "\n",
    "\n",
    "\n",
    "# Function to check if a job is already done\n",
    "is_job_done() {\n",
    "    job_number=$1\n",
    "    grep -q \"Job $job_number: DONE\" \"$LOGFILE\"\n",
    "}\n",
    "\n",
    "# Function to mark a job as done\n",
    "mark_job_done() {\n",
    "    job_number=$1\n",
    "    echo \"Job $job_number: DONE\" >> \"$LOGFILE\"\n",
    "}\n",
    "\n",
    "# Start executing jobs\n",
    "for i in \"${!jobs[@]}\"; do\n",
    "    job_number=$((i + 1))\n",
    "    job=\"${jobs[$i]}\"\n",
    "\n",
    "    if is_job_done $job_number; then\n",
    "        echo \"Job $job_number is already done. Skipping.\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    # Run the job and log its output\n",
    "    echo \"Starting Job $job_number: $job\"\n",
    "    if eval \"$job\" >> \"$OUTPUT_LOG\" 2>&1; then\n",
    "        echo \"Job $job_number completed successfully.\"\n",
    "        mark_job_done $job_number\n",
    "    else\n",
    "        echo \"Job $job_number failed. Check $OUTPUT_LOG for details.\"\n",
    "        exit 1\n",
    "    fi\n",
    "done\n",
    "\n",
    "echo \"All jobs completed.\"\n",
    "\n",
    "''')\n",
    "\n",
    "bf.close()\n",
    "print(job_num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test load and unload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get simulation batch file \n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "job_name = 'test_load_unload'\n",
    "\n",
    "infile_dir = 'infile'\n",
    "data_dir_ = 'spinodal_model_dat'\n",
    "\n",
    "di_strenght = [0.2,0.4,0.6,0.8,1.0]\n",
    "\n",
    "\n",
    "bf = open(f'batch_files/{job_name}.sh','w')\n",
    "bf.write(f'''#!/bin/bash\n",
    "\n",
    "#SBATCH --partition=batch   # Partition name (batch, highmem_p, or gpu_p)\n",
    "##SBATCH --gres=gpu:A100:1\n",
    "#SBATCH --nodes=1\n",
    "#SBATCH --ntasks=64\n",
    "#SBATCH --cpus-per-task=1\n",
    "#SBATCH --mem=128G\n",
    "#SBATCH --time=2-00:00:00\n",
    "#SBATCH --job-name={job_name}\n",
    "#SBATCH --mail-user=jt35560@uga.edu   # Mair eves BOIN, END, FAIL, ALL)\n",
    "#SBATCH --mail-type=ALL\n",
    "#SBATCH --output=/home/jt35560/tmp/slurm_log/%j-%x-%N.out\n",
    "\n",
    "source /home/jt35560/.bashrc\n",
    "source /home/jt35560/.local/module/lmp.sh\n",
    "conda activate py311 || exit\n",
    "ml OpenMPI\n",
    "\n",
    "cd /home/jt35560/codebase/prj3/batch_files\n",
    "\n",
    "# Log file to keep track of completed jobs\n",
    "LOGFILE=\"{job_name}_log.txt\"\n",
    "# Check if log file exists, if not, create one\n",
    "if [[ ! -f \"$LOGFILE\" ]]; then\n",
    "    touch \"$LOGFILE\"\n",
    "fi\n",
    "\n",
    "# Output log for the jobs\n",
    "OUTPUT_LOG=\"{job_name}_output_log.txt\"\n",
    "\n",
    "if [[ ! -f \"$OUTPUT_LOG\" ]]; then\n",
    "    touch \"$OUTPUT_LOG\"\n",
    "fi\n",
    "\n",
    "declare -a jobs=(\n",
    "''')\n",
    "\n",
    "job_num = 0\n",
    "import os\n",
    "for comp in np.arange(10,70,10):\n",
    "    for ds in di_strenght:\n",
    "        data_dir= data_dir_ +f'_{ds}'\n",
    "        for iroot,idirs,ifiles in os.walk(infile_dir):\n",
    "            if len(iroot.split('/'))!=1:\n",
    "                continue\n",
    "            for ifile in ifiles:\n",
    "                if ifile[0] != '.' and 'compress' in ifile and 'relax' in ifile:\n",
    "                    in_file = os.path.join(iroot,ifile)\n",
    "                    in_file = os.path.abspath(in_file)\n",
    "                    c = 0\n",
    "                    for droot, ddirs,dfiles in os.walk(data_dir):\n",
    "                        for dfile in dfiles:\n",
    "                            if dfile[0] != '.' and dfile.endswith('.dat'):\n",
    "                                data = os.path.join(droot,dfile)[:-4]\n",
    "                                data = os.path.abspath(data)\n",
    "                                o_droot= os.path.abspath(droot)\n",
    "                                o_droot = o_droot.replace('home','scratch')\n",
    "                                o_data = os.path.join(o_droot+f'/{job_name}',dfile)[:-4]\n",
    "                                os.system(f'mkdir -p {o_droot+f\"/{job_name}\"}')\n",
    "                                c+=1\n",
    "                                cmd = f'mpirun -np 64 tian_jie_lmp -i {in_file} -var file {data} -var ofile {o_data} -var comp {comp}'\n",
    "                                print(cmd)\n",
    "                                bf.write(\"\\\"\"+cmd+\"\\\"\\n\")\n",
    "                                job_num+=1\n",
    "                                # os.system(cmd)\n",
    "                                if c>=5:\n",
    "                                    break\n",
    "                                    \n",
    "bf.write(')\\n')\n",
    "\n",
    "\n",
    "bf.write('''\n",
    "\n",
    "\n",
    "\n",
    "# Function to check if a job is already done\n",
    "is_job_done() {\n",
    "    job_number=$1\n",
    "    grep -q \"Job $job_number: DONE\" \"$LOGFILE\"\n",
    "}\n",
    "\n",
    "# Function to mark a job as done\n",
    "mark_job_done() {\n",
    "    job_number=$1\n",
    "    echo \"Job $job_number: DONE\" >> \"$LOGFILE\"\n",
    "}\n",
    "\n",
    "# Start executing jobs\n",
    "for i in \"${!jobs[@]}\"; do\n",
    "    job_number=$((i + 1))\n",
    "    job=\"${jobs[$i]}\"\n",
    "\n",
    "    if is_job_done $job_number; then\n",
    "        echo \"Job $job_number is already done. Skipping.\"\n",
    "        continue\n",
    "    fi\n",
    "\n",
    "    # Run the job and log its output\n",
    "    echo \"Starting Job $job_number: $job\"\n",
    "    if eval \"$job\" >> \"$OUTPUT_LOG\" 2>&1; then\n",
    "        echo \"Job $job_number completed successfully.\"\n",
    "        mark_job_done $job_number\n",
    "    else\n",
    "        echo \"Job $job_number failed. Check $OUTPUT_LOG for details.\"\n",
    "        exit 1\n",
    "    fi\n",
    "done\n",
    "\n",
    "echo \"All jobs completed.\"\n",
    "\n",
    "''')\n",
    "\n",
    "bf.close()\n",
    "print(job_num)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# post\n",
    "\n",
    "import ovito \n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def post(file):\n",
    "    pipline = ovito.io.import_file(file)\n",
    "    frame_num = pipline.source.num_frames\n",
    "    # print(frame_num)\n",
    "    S = np.zeros((frame_num, 24))\n",
    "\n",
    "    for i in range(frame_num):\n",
    "        data = pipline.compute(i)\n",
    "        cell = np.array(data.cell)\n",
    "        # print(cell)\n",
    "        voll = cell[:,:3].diagonal()\n",
    "        vol = np.prod(voll)\n",
    "        vxyz = cell[:,3].flatten()\n",
    "        # print(vxyz)\n",
    "        # print(vol)\n",
    "        # calculate the stress tensor\n",
    "        sxx = data.particles['c_1[1]'].array\n",
    "        syy = data.particles['c_1[2]'].array\n",
    "        szz = data.particles['c_1[3]'].array\n",
    "        sxy = data.particles['c_1[4]'].array\n",
    "        sxz = data.particles['c_1[5]'].array\n",
    "        syz = data.particles['c_1[6]'].array\n",
    "        \n",
    "        sxxb = data.particles['c_b[1]'].array\n",
    "        syyb = data.particles['c_b[2]'].array\n",
    "        szzb = data.particles['c_b[3]'].array\n",
    "        sxyb = data.particles['c_b[4]'].array\n",
    "        sxzb = data.particles['c_b[5]'].array\n",
    "        syzb = data.particles['c_b[6]'].array\n",
    "        \n",
    "        sxxd = data.particles['c_d[1]'].array\n",
    "        syyd = data.particles['c_d[2]'].array\n",
    "        szzd = data.particles['c_d[3]'].array\n",
    "        sxyd = data.particles['c_d[4]'].array\n",
    "        sxzd = data.particles['c_d[5]'].array\n",
    "        syzd = data.particles['c_d[6]'].array\n",
    "        \n",
    "        \n",
    "        pxx = np.sum(sxx) /(vol * 1e4)\n",
    "        pyy = np.sum(syy) /(vol * 1e4)\n",
    "        pzz = np.sum(szz) /(vol * 1e4)\n",
    "        pxy = np.sum(sxy) /(vol * 1e4)\n",
    "        pxz = np.sum(sxz) /(vol * 1e4)\n",
    "        pyz = np.sum(syz) /(vol * 1e4)\n",
    "        \n",
    "        pxxb = np.sum(sxxb) /(vol * 1e4)\n",
    "        pyyb = np.sum(syyb) /(vol * 1e4)\n",
    "        pzzb = np.sum(szzb) /(vol * 1e4)\n",
    "        pxyb = np.sum(sxyb) /(vol * 1e4)\n",
    "        pxzb = np.sum(sxzb) /(vol * 1e4)\n",
    "        pyzb = np.sum(syzb) /(vol * 1e4)\n",
    "        \n",
    "        \n",
    "        pxxd = np.sum(sxxd) /(vol * 1e4)\n",
    "        pyyd = np.sum(syyd) /(vol * 1e4)\n",
    "        pzzd = np.sum(szzd) /(vol * 1e4)\n",
    "        pxyd = np.sum(sxyd) /(vol * 1e4)\n",
    "        pxzd = np.sum(sxzd) /(vol * 1e4)\n",
    "        pyzd = np.sum(syzd) /(vol * 1e4)\n",
    "        \n",
    "        \n",
    "        \n",
    "        S[i] = [voll[0], voll[1], voll[2], vxyz[0], vxyz[1], vxyz[2], pxx, pyy, pzz, pxy, pxz, pyz, pxxb, pyyb, pzzb, pxyb, pxzb, pyzb, pxxd, pyyd, pzzd, pxyd, pxzd, pyzd]\n",
    "\n",
    "    pd.DataFrame(S).to_csv(file+'.csv',header=['lx','ly','lz','lxy','lxz','lyz','sxx','syy','szz','sxy','sxz','syz','sxxb','syyb','szzb','sxyb','sxzb','syzb','sxxd','syyd','szzd','sxyd','sxzd','syzd'],index=False)\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "for root,dirs,files in os.walk('spinodal_model_dat'):\n",
    "    for file in files:\n",
    "        if file.endswith('dump') and file[0] != '.':\n",
    "            if not 'center' in file:\n",
    "                file = os.path.join(root,file)\n",
    "                print(file)\n",
    "                post(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from plot_layout import layout\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "csv_dir = 'csv_res'\n",
    "\n",
    "tb = pd.read_csv('csv_res/0.0_0.0_0.0_10.stl.x.dump.csv',sep=',')\n",
    "eps = (tb['lx'] - tb['lx'][0])/tb['lx'][0]\n",
    "eps = -np.array(eps)\n",
    "for root, dirs,files in os.walk(csv_dir):\n",
    "    for file in files:\n",
    "        if file.endswith('csv') and file[0]!='.':\n",
    "            res = os.path.join(root, file)\n",
    "            print(res)\n",
    "            orient = file.split('.')[-3]\n",
    "            if len(orient) == 1:\n",
    "                sorient = orient+orient\n",
    "            else:\n",
    "                sorient = orient\n",
    "            data = pd.read_csv(res,sep=',')\n",
    "            data_y = data['s'+sorient]\n",
    "            data_y = data_y-data_y[0]\n",
    "            scatter = go.Scatter(x=eps[5:],y=-data_y[5:],mode='markers+lines',name='tol')\n",
    "            scatterb = go.Scatter(x=eps[5:],y=-data['s'+sorient+'b'][5:],mode='markers+lines',name='bond')\n",
    "            scatterd = go.Scatter(x=eps[5:],y=-data['s'+sorient+'d'][5:],mode='markers+lines',name='dihedral')\n",
    "            \n",
    "            fig = go.Figure([scatter,scatterb,scatterd])\n",
    "            layout['yaxis']['title']['text'] = '<b>Stress(Gpa)</b>'\n",
    "            layout['yaxis']['title']['text'] = '<b>Strain</b>'\n",
    "            layout['showlegend'] = True\n",
    "            fig.update_layout(layout)\n",
    "            # fig.show()\n",
    "            fig.write_image(res[:-4]+'.png')\n",
    "            # break\n",
    "            \n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Post"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
